"""
è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è·å–ä»»åŠ¡
è·å–Aè‚¡çš„åŸºæœ¬ä¿¡æ¯å¹¶åŒæ­¥åˆ°æœ¬åœ°æ•°æ®åº“
"""
import asyncio
from datetime import datetime
from typing import Dict, List, Optional

import tushare as ts
import httpx
import pandas as pd

from app.core.config import settings
from app.utils.logger import logger

class StockDataFetcher:
    """è‚¡ç¥¨æ•°æ®è·å–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.last_fetch_time: Optional[datetime] = None
        self.api_base_url = f"http://{settings.stock_api_host}:{settings.stock_api_port}/api"
        self.batch_size = settings.stock_batch_size
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {settings.stock_api_token}"
        }
        # åˆå§‹åŒ– tushare pro
        self.tushare_token = "347ae3b92b9a97638f155512bc599767558b94c3dcb47f5abd058b95"
        ts.set_token(self.tushare_token)
        self.pro = ts.pro_api()

    async def fetch_all_stock_info(self):
        """
        è·å–Aè‚¡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¹¶åŒæ­¥åˆ°æ•°æ®åº“
        æ¯å¤©å‡Œæ™¨00:00æ‰§è¡Œä¸€æ¬¡

        æµç¨‹ï¼š
        1. è°ƒç”¨åç«¯æŸ¥è¯¢æ¥å£ï¼ˆ1æ¬¡ï¼‰ - è·å–å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç 
        2. è°ƒç”¨ Tushare è‚¡ç¥¨åˆ—è¡¨æ¥å£ï¼ˆ1æ¬¡ï¼‰ - è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä¿¡æ¯
        3. å¯¹æ¯”å·®å¼‚ï¼Œæ‰¾å‡ºéœ€è¦æ’å…¥çš„æ–°è‚¡ç¥¨
        4. è°ƒç”¨åç«¯æ‰¹é‡æ’å…¥æ¥å£ï¼ˆè‹¥å¹²æ¬¡ï¼‰ - åªæ’å…¥æ–°è‚¡ç¥¨
        """
        logger.info("=" * 80)
        logger.info("å¼€å§‹åŒæ­¥è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
        logger.info(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)

        try:
            # 1. æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç ï¼ˆåªæŸ¥ä»£ç ï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
            logger.info("ğŸ“Š æ­¥éª¤1: æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç ...")
            existing_codes = await self._query_existing_stock_codes()
            logger.info(f"âœ“ æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_codes)} æ”¯è‚¡ç¥¨")

            # 2. ä» Tushare Pro è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä¿¡æ¯
            logger.info("\nğŸ“Š æ­¥éª¤2: ä» Tushare Pro è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨ä¿¡æ¯...")
            all_stocks = await self._fetch_a_share_info()
            logger.info(f"âœ“ ä» Tushare Pro è·å–åˆ° {len(all_stocks)} æ”¯Aè‚¡è‚¡ç¥¨")

            # 3. å¯¹æ¯”å·®å¼‚ï¼Œæ‰¾å‡ºéœ€è¦æ’å…¥çš„æ–°è‚¡ç¥¨
            logger.info("\nğŸ“Š æ­¥éª¤3: å¯¹æ¯”å·®å¼‚ï¼Œç­›é€‰éœ€è¦æ’å…¥çš„æ–°è‚¡ç¥¨...")
            stocks_to_insert = [
                stock for stock in all_stocks
                if stock["stockCode"] not in existing_codes
            ]
            logger.info(f"âœ“ å‘ç° {len(stocks_to_insert)} æ”¯æ–°è‚¡ç¥¨éœ€è¦æ’å…¥")

            # 4. åˆ†æ‰¹æ’å…¥æ–°è‚¡ç¥¨
            if stocks_to_insert:
                logger.info(f"\nğŸ“Š æ­¥éª¤4: åˆ†æ‰¹æ’å…¥æ–°è‚¡ç¥¨ï¼ˆæ¯æ‰¹{self.batch_size}æ¡ï¼‰...")
                await self._batch_insert_stocks(stocks_to_insert)
            else:
                logger.info("\nâœ“ æ²¡æœ‰æ–°è‚¡ç¥¨éœ€è¦æ’å…¥ï¼Œæ•°æ®å·²æ˜¯æœ€æ–°")

            self.last_fetch_time = datetime.now()
            logger.info("\n" + "=" * 80)
            logger.info("âœ“ Aè‚¡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯åŒæ­¥å®Œæˆï¼")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"è‚¡ç¥¨ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            raise

    async def _query_existing_stock_codes(self) -> set:
        """
        æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç ï¼ˆåªæŸ¥ä»£ç ï¼Œä¸æŸ¥å…¨éƒ¨å­—æ®µï¼‰

        Returns:
            set: å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç é›†åˆ
        """
        try:
            async with httpx.AsyncClient(timeout=None) as client:
                response = await client.post(
                    f"{self.api_base_url}/stocks/query",
                    json={
                        "exchanges": ["SSE", "SZSE", "BSE"]
                    },
                    headers=self.headers
                )
                response.raise_for_status()

                result = response.json()
                if result.get("code") == 200:
                    stocks = result.get("data", [])
                    # åªæå–è‚¡ç¥¨ä»£ç åˆ°é›†åˆä¸­ï¼Œé‡Šæ”¾å®Œæ•´æ•°æ®
                    return {stock["stockCode"] for stock in stocks}
                else:
                    logger.error(f"æŸ¥è¯¢è‚¡ç¥¨å¤±è´¥: {result.get('message')}")
                    return set()

        except httpx.HTTPError as e:
            logger.error(f"æŸ¥è¯¢è‚¡ç¥¨æ¥å£è¯·æ±‚å¤±è´¥: {e}")
            return set()
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è‚¡ç¥¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return set()

    async def _fetch_a_share_info(self) -> List[Dict]:
        """
        ä» Tushare Pro è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

        Returns:
            List[Dict]: è‚¡ç¥¨ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡ŒåŒæ­¥çš„ tushare è°ƒç”¨
            loop = asyncio.get_event_loop()

            # ä½¿ç”¨ tushare pro çš„ stock_basic æ¥å£è·å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨
            # è·å–æ‰€æœ‰çŠ¶æ€
            df = await loop.run_in_executor(
                None,
                lambda: self.pro.stock_basic(
                    fields='ts_code,symbol,name,area,industry,list_date,fullname,enname,cnspell,market,exchange,curr_type,list_status,delist_date,is_hs,act_name,act_ent_type'
                )
            )

            if df is None or df.empty:
                logger.warning("æœªè·å–åˆ°Aè‚¡æ•°æ®")
                return []

            stocks = []
            for _, row in df.iterrows():
                try:
                    # tushare è¿”å›çš„å­—æ®µæ˜ å°„
                    ts_code = str(row.get("ts_code", ""))   # æ ¼å¼ï¼š000001.SZ
                    exchange = str(row.get("exchange", ""))  # SSE/SZSE/BSE

                    if not ts_code or not exchange:
                        continue

                    # å¤„ç†æ—¥æœŸæ ¼å¼ï¼štushare è¿”å› YYYYMMDD æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸º YYYY-MM-DD
                    list_date = str(row.get("list_date", ""))
                    if list_date and len(list_date) == 8:
                        list_date = f"{list_date[:4]}-{list_date[4:6]}-{list_date[6:]}"
                    else:
                        list_date = None

                    delist_date = str(row.get("delist_date", ""))
                    if delist_date and len(delist_date) == 8:
                        delist_date = f"{delist_date[:4]}-{delist_date[4:6]}-{delist_date[6:]}"
                    else:
                        delist_date = None

                    # æ„å»ºè‚¡ç¥¨ä¿¡æ¯å­—å…¸
                    stock_info = {
                        "exchange": exchange,
                        "stockCode": ts_code,
                        "stockName": str(row.get("name", "")),
                        "area": str(row.get("area", "")) if row.get("area") else None,
                        "industry": str(row.get("industry", "")) if row.get("industry") else None,
                        "listingDate": list_date,
                        "fullName": str(row.get("fullname", "")) if row.get("fullname") else None,
                        "enName": str(row.get("enname", "")) if row.get("enname") else None,
                        "cnSpell": str(row.get("cnspell", "")) if row.get("cnspell") else None,
                        "market": str(row.get("market", "")) if row.get("market") else None,
                        "currType": str(row.get("curr_type", "")) if row.get("curr_type") else None,
                        "status": str(row.get("list_status", "")),
                        "delistDate": delist_date,
                        "isHs": str(row.get("is_hs", "")) if row.get("is_hs") else None,
                        "actName": str(row.get("act_name", "")) if row.get("act_name") else None,
                        "actEntType": str(row.get("act_ent_type", "")) if row.get("act_ent_type") else None,
                    }

                    stocks.append(stock_info)

                except Exception as e:
                    logger.warning(f"è§£æè‚¡ç¥¨æ•°æ®å¤±è´¥: {row}, é”™è¯¯: {e}")
                    continue

            logger.info(f"æˆåŠŸä» Tushare Pro è·å– {len(stocks)} æ”¯Aè‚¡è‚¡ç¥¨ä¿¡æ¯")
            return stocks

        except Exception as e:
            logger.error(f"è·å–Aè‚¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return []

    async def _batch_insert_stocks(self, stocks: List[Dict]):
        """
        åˆ†æ‰¹æ¬¡æ’å…¥è‚¡ç¥¨æ•°æ®

        Args:
            stocks: å¾…æ’å…¥çš„è‚¡ç¥¨åˆ—è¡¨
        """
        total = len(stocks)
        batches = (total + self.batch_size - 1) // self.batch_size  # å‘ä¸Šå–æ•´

        success_count = 0
        fail_count = 0

        async with httpx.AsyncClient(timeout=None) as client:
            for i in range(batches):
                start_idx = i * self.batch_size
                end_idx = min((i + 1) * self.batch_size, total)
                batch = stocks[start_idx:end_idx]

                logger.info(f"æ­£åœ¨æ’å…¥ç¬¬ {i+1}/{batches} æ‰¹ï¼Œå…± {len(batch)} æ¡è®°å½•...")

                try:
                    response = await client.post(
                        f"{self.api_base_url}/stocks/batch",
                        json=batch,
                        headers=self.headers
                    )
                    response.raise_for_status()

                    result = response.json()
                    if result.get("code") == 200:
                        success_count += len(batch)
                        logger.info(f"âœ“ ç¬¬ {i+1} æ‰¹æ’å…¥æˆåŠŸï¼Œå·²ç´¯è®¡æˆåŠŸ {success_count}/{total} æ¡")
                    else:
                        fail_count += len(batch)
                        logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥å¤±è´¥: {result.get('message')}")

                except httpx.HTTPError as e:
                    fail_count += len(batch)
                    logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥è¯·æ±‚å¤±è´¥: {e}")
                except Exception as e:
                    fail_count += len(batch)
                    logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")

                # é¿å…è¯·æ±‚è¿‡å¿«ï¼Œç¨ä½œå»¶è¿Ÿ
                if i < batches - 1:
                    await asyncio.sleep(0.5)

        logger.info(f"\næ‰¹é‡æ’å…¥å®Œæˆ: æˆåŠŸ {success_count} æ¡, å¤±è´¥ {fail_count} æ¡")

    async def fetch_all_company_info(self):
        """
        è·å–å…¨é‡ä¸Šå¸‚å…¬å¸åŸºæœ¬ä¿¡æ¯å¹¶åŒæ­¥åˆ°æ•°æ®åº“
        æ¯å‘¨æœ«å‡Œæ™¨1:00æ‰§è¡Œä¸€æ¬¡

        æµç¨‹ï¼š
        1. ä» Tushare Pro åˆ†æ‰¹è·å–å…¨é‡å…¬å¸åŸºæœ¬ä¿¡æ¯ï¼ˆæ¯æ¬¡4500æ¡ï¼‰
        2. åˆ†æ‰¹è°ƒç”¨åç«¯æ‰¹é‡æ’å…¥æ¥å£ï¼ˆæ¯æ‰¹1000æ¡ï¼‰
        """
        logger.info("=" * 80)
        logger.info("å¼€å§‹åŒæ­¥ä¸Šå¸‚å…¬å¸åŸºæœ¬ä¿¡æ¯...")
        logger.info(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)

        try:
            # 1. ä» Tushare Pro è·å–å…¨é‡å…¬å¸åŸºæœ¬ä¿¡æ¯
            logger.info("\nğŸ“Š æ­¥éª¤1: ä» Tushare Pro è·å–å…¨é‡å…¬å¸åŸºæœ¬ä¿¡æ¯...")
            all_companies = await self._fetch_all_company_info_from_tushare()
            logger.info(f"âœ“ ä» Tushare Pro è·å–åˆ° {len(all_companies)} å®¶å…¬å¸ä¿¡æ¯")

            # 2. åˆ†æ‰¹è°ƒç”¨åç«¯æ‰¹é‡æ’å…¥æ¥å£
            if all_companies:
                logger.info(f"\nğŸ“Š æ­¥éª¤2: åˆ†æ‰¹æ’å…¥å…¬å¸ä¿¡æ¯ï¼ˆæ¯æ‰¹1000æ¡ï¼‰...")
                await self._batch_upsert_companies(all_companies)
            else:
                logger.info("\nâœ“ æ²¡æœ‰å…¬å¸ä¿¡æ¯éœ€è¦åŒæ­¥")

            self.last_fetch_time = datetime.now()
            logger.info("\n" + "=" * 80)
            logger.info("âœ“ ä¸Šå¸‚å…¬å¸åŸºæœ¬ä¿¡æ¯åŒæ­¥å®Œæˆï¼")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"å…¬å¸ä¿¡æ¯åŒæ­¥å¤±è´¥: {e}", exc_info=True)
            raise

    async def _fetch_all_company_info_from_tushare(self) -> List[Dict]:
        """
        ä» Tushare Pro åˆ†æ‰¹è·å–å…¨é‡å…¬å¸åŸºæœ¬ä¿¡æ¯
        æ¯æ¬¡è¯·æ±‚4500æ¡ï¼Œç›´åˆ°å…¨éƒ¨è·å–

        Returns:
            List[Dict]: å…¬å¸ä¿¡æ¯åˆ—è¡¨
        """
        try:
            loop = asyncio.get_event_loop()
            all_companies = []
            offset = 0
            limit = 4500  # æ¯æ¬¡è¯·æ±‚4500æ¡

            while True:
                logger.info(f"æ­£åœ¨è·å–ç¬¬ {offset} - {offset + limit} æ¡å…¬å¸ä¿¡æ¯...")

                # è°ƒç”¨ Tushare Pro çš„ stock_company æ¥å£
                # æ³¨æ„ï¼šå¿…é¡»åŒ…å« com_name å’Œ com_id å­—æ®µ
                current_offset = offset
                current_limit = limit
                df = await loop.run_in_executor(
                    None,
                    lambda: self.pro.stock_company(
                        fields='ts_code,com_name,com_id,exchange,chairman,manager,secretary,reg_capital,setup_date,province,city,introduction,website,email,office,employees,main_business,business_scope',
                        limit=current_limit,
                        offset=current_offset
                    )
                )

                if df is None or df.empty:
                    logger.info(f"âœ“ å·²è·å–å…¨éƒ¨å…¬å¸ä¿¡æ¯ï¼Œå…± {len(all_companies)} æ¡")
                    break

                # è§£ææ•°æ®
                for _, row in df.iterrows():
                    try:
                        ts_code = str(row.get("ts_code", ""))
                        if not ts_code:
                            continue

                        # å¤„ç†æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD -> YYYY-MM-DD
                        setup_date = str(row.get("setup_date", ""))
                        if setup_date and len(setup_date) == 8:
                            setup_date = f"{setup_date[:4]}-{setup_date[4:6]}-{setup_date[6:]}"
                        else:
                            setup_date = None

                        # ç›´æ¥ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­è·å–å…¬å¸åç§°å’ŒID
                        com_name = str(row.get("com_name", "")) if pd.notna(row.get("com_name")) else ts_code
                        com_id = str(row.get("com_id", "")) if pd.notna(row.get("com_id")) else ts_code

                        # å¤„ç†æ•°å€¼ç±»å‹å­—æ®µï¼ˆå¯èƒ½ä¸º NaNï¼‰
                        reg_capital = row.get("reg_capital")
                        reg_capital = float(reg_capital) if pd.notna(reg_capital) else None

                        employees = row.get("employees")
                        employees = int(employees) if pd.notna(employees) else None

                        # æ„å»ºå…¬å¸ä¿¡æ¯å­—å…¸
                        company_info = {
                            "stockCode": ts_code,
                            "comName": com_name,
                            "comId": com_id,
                            "exchange": str(row.get("exchange", "")) if pd.notna(row.get("exchange")) else "",
                            "chairman": str(row.get("chairman", "")) if pd.notna(row.get("chairman")) else "",
                            "manager": str(row.get("manager", "")) if pd.notna(row.get("manager")) else None,
                            "secretary": str(row.get("secretary", "")) if pd.notna(row.get("secretary")) else None,
                            "regCapital": reg_capital,
                            "setupDate": setup_date,
                            "province": str(row.get("province", "")) if pd.notna(row.get("province")) else None,
                            "city": str(row.get("city", "")) if pd.notna(row.get("city")) else None,
                            "introduction": str(row.get("introduction", "")) if pd.notna(row.get("introduction")) else None,
                            "website": str(row.get("website", "")) if pd.notna(row.get("website")) else None,
                            "email": str(row.get("email", "")) if pd.notna(row.get("email")) else None,
                            "office": str(row.get("office", "")) if pd.notna(row.get("office")) else None,
                            "employees": employees,
                            "mainBusiness": str(row.get("main_business", "")) if pd.notna(row.get("main_business")) else None,
                            "businessScope": str(row.get("business_scope", "")) if pd.notna(row.get("business_scope")) else None,
                        }

                        all_companies.append(company_info)

                    except Exception as e:
                        logger.warning(f"è§£æå…¬å¸æ•°æ®å¤±è´¥: {row}, é”™è¯¯: {e}")
                        continue

                logger.info(f"âœ“ è·å–åˆ° {len(df)} æ¡è®°å½•ï¼Œç´¯è®¡ {len(all_companies)} æ¡")

                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºè¯·æ±‚çš„æ•°é‡ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€æ‰¹
                if len(df) < limit:
                    logger.info(f"âœ“ å·²è·å–å…¨éƒ¨å…¬å¸ä¿¡æ¯ï¼Œå…± {len(all_companies)} æ¡")
                    break

                # æ›´æ–°åç§»é‡
                offset += limit

                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)

            return all_companies

        except Exception as e:
            logger.error(f"ä» Tushare Pro è·å–å…¬å¸ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            return []

    async def _batch_upsert_companies(self, companies: List[Dict]):
        """
        åˆ†æ‰¹æ¬¡æ‰¹é‡æ’å…¥/æ›´æ–°å…¬å¸ä¿¡æ¯
        æ¯æ‰¹1000æ¡

        Args:
            companies: å¾…æ’å…¥çš„å…¬å¸ä¿¡æ¯åˆ—è¡¨
        """
        total = len(companies)
        batch_size = 1000
        batches = (total + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´

        success_count = 0
        fail_count = 0

        async with httpx.AsyncClient(timeout=None) as client:
            for i in range(batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, total)
                batch = companies[start_idx:end_idx]

                logger.info(f"æ­£åœ¨æ’å…¥ç¬¬ {i+1}/{batches} æ‰¹å…¬å¸ä¿¡æ¯ï¼Œå…± {len(batch)} æ¡è®°å½•...")

                try:
                    response = await client.post(
                        f"{self.api_base_url}/stock-companies/batch-upsert",
                        json=batch,
                        headers=self.headers
                    )
                    response.raise_for_status()

                    result = response.json()
                    if result.get("code") == 200:
                        success_count += len(batch)
                        logger.info(f"âœ“ ç¬¬ {i+1} æ‰¹æ’å…¥æˆåŠŸï¼Œå·²ç´¯è®¡æˆåŠŸ {success_count}/{total} æ¡")
                    else:
                        fail_count += len(batch)
                        logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥å¤±è´¥: {result.get('message')}")

                except httpx.HTTPError as e:
                    fail_count += len(batch)
                    logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥è¯·æ±‚å¤±è´¥: {e}")
                except Exception as e:
                    fail_count += len(batch)
                    logger.error(f"âœ— ç¬¬ {i+1} æ‰¹æ’å…¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")

                # é¿å…è¯·æ±‚è¿‡å¿«ï¼Œç¨ä½œå»¶è¿Ÿ
                if i < batches - 1:
                    await asyncio.sleep(0.5)

        logger.info(f"\næ‰¹é‡æ’å…¥å®Œæˆ: æˆåŠŸ {success_count} æ¡, å¤±è´¥ {fail_count} æ¡")
